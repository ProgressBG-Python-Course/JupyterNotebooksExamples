{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "docA = \"apple apple apple apple a banana an banana \"\n",
    "docB = \"apple car a car\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "### Represent each document as 'Bag of words' (bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize\n",
    "\n",
    "In NLP (Natural Language Processing), tokenization is the process of breaking down a piece of text (such as a sentence or paragraph) into smaller units, called tokens. \n",
    "\n",
    "These tokens can be words, subwords, or even characters, depending on the level of tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['apple', 'apple', 'apple', 'apple', 'a', 'banana', 'an', 'banana']\n",
      "['apple', 'car', 'a', 'car']\n"
     ]
    }
   ],
   "source": [
    "# for simplicity, we will tokenize our documents assuming everything between\n",
    "# whitespaces is a word.\n",
    "# in reality you may want to use nltk.word_tokenize()\n",
    "tokensA = docA.split()\n",
    "tokensB = docB.split()\n",
    "print( tokensA )\n",
    "print( tokensB )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "\n",
    "Stemming in NLP (Natural Language Processing) is the process of reducing a word to its base or root form, called a \"stem.\"\n",
    "\n",
    "'apple' and 'apples' should be counted as a same term. Stemming is helping that process.\n",
    "Resource: https://pythonspot.com/nltk-stemming/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['appl', 'appl', 'appl', 'appl', 'a', 'banana', 'an', 'banana']\n",
      "['appl', 'car', 'a', 'car']\n"
     ]
    }
   ],
   "source": [
    "# Initialize the stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Apply stemming to the tokens\n",
    "stemmedA = [stemmer.stem(token) for token in tokensA]\n",
    "stemmedB = [stemmer.stem(token) for token in tokensB]\n",
    "\n",
    "print(stemmedA)\n",
    "print(stemmedB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove the 'stop words'\n",
    "\n",
    "Prepositions, articles and other common words are considered as useless (low lexical content) in document representation, thus you can filter the \"Bag of Words\" through a common stop-word list, or create custom specific.\n",
    "NLTK module has a list of stopewrods for many languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['appl', 'appl', 'appl', 'appl', 'banana', 'banana']\n",
      "['appl', 'car', 'car']\n"
     ]
    }
   ],
   "source": [
    "# Get the list of English stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "bowA = [token for token in stemmedA if token.lower() not in stop_words]\n",
    "bowB = [token for token in stemmedB if token.lower() not in stop_words]\n",
    "\n",
    "print(bowA)\n",
    "print(bowB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the collection vocabulary\n",
    "\n",
    "Make one list (set) of unique words for document collection\n",
    "\n",
    "That set will represent the vocabulary of our documents collection\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary: {'banana', 'appl', 'car'}\n"
     ]
    }
   ],
   "source": [
    "vocabulary = set(bowA).union(set(bowB))\n",
    "print(f'vocabulary: {vocabulary}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count unique words in each document, i.e. Term Frequency (TF)\n",
    "\n",
    "Term frequency indicates the significance of a particular term in the document\n",
    "\n",
    "Now, we have to represent each document as numbers of terms occurrence in the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFa:{'banana': 0, 'appl': 0, 'car': 0}\n"
     ]
    }
   ],
   "source": [
    "# Initialize the dictionaries to store Term Frequencies:\n",
    "TFa = dict.fromkeys(vocabulary, 0)\n",
    "TFb = dict.fromkeys(vocabulary, 0)\n",
    "\n",
    "print(f'TFa:{TFa}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "countsA: {'banana': 2, 'appl': 4, 'car': 0}\n",
      "countsB: {'banana': 0, 'appl': 1, 'car': 2}\n"
     ]
    }
   ],
   "source": [
    "# Calculate term frequences:\n",
    "for word in vocabulary:\n",
    "    TFa[word] = bowA.count(word)\n",
    "    TFb[word] = bowB.count(word)\n",
    "\n",
    "print(f'countsA: {TFa}')\n",
    "print(f'countsB: {TFb}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create DataFrame to store document colection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>banana</th>\n",
       "      <th>appl</th>\n",
       "      <th>car</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   banana  appl  car\n",
       "0       2     4    0\n",
       "1       0     1    2"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts_df = pd.DataFrame([TFa, TFb])\n",
    "counts_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate adjusted TF\n",
    "\n",
    "**Adjusted term frequency for a document** is calculated by dividing the number of occurrences of the term by the total number of terms in that document:\n",
    "\n",
    "Adjusted Term Frequency = Count of the term / Total number of terms in the document\n",
    "\n",
    "This gives the relative frequency of the term within the document.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>banana</th>\n",
       "      <th>appl</th>\n",
       "      <th>car</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     banana      appl       car\n",
       "0  0.333333  0.666667  0.000000\n",
       "1  0.000000  0.333333  0.666667"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the total number of terms in each document (sum of each row)\n",
    "total_terms_per_doc = counts_df.sum(axis=1)\n",
    "\n",
    "# Calculate the adjusted term frequency (TF) by dividing each count by the total number of terms in that document\n",
    "TF_df = counts_df.div(total_terms_per_doc, axis=0)\n",
    "TF_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute IDF\n",
    "\n",
    "**IDF (Inverse Document Frequency)** is a measure used in text processing, particularly in **TF-IDF (Term Frequency-Inverse Document Frequency)**, to evaluate how important a word is to a document within a collection or corpus.\n",
    "\n",
    "### Key Idea:\n",
    "IDF helps determine whether a term is **common or rare** across multiple documents in a corpus:\n",
    "- Words that appear in **many documents** are considered **less important** (less informative) because they are common.\n",
    "- Words that appear in **fewer documents** are considered **more important** because they provide more unique information.\n",
    "\n",
    "### Formula for IDF:\n",
    "$$\n",
    "IDF(t) = \\log \\left( \\frac{N}{1 + df(t)} \\right)\n",
    "$$\n",
    "Where:\n",
    "- \\(N\\) = total number of documents in the corpus.\n",
    "- \\(df(t)\\) = number of documents containing the term \\(t\\).\n",
    "- \\(1\\) is added in the denominator to avoid division by zero in case the term doesn’t appear in any document.\n",
    "\n",
    "### Explanation:\n",
    "- If a term appears in **all documents**, its IDF will be low (often close to 0), indicating that it is not very informative.\n",
    "- If a term appears in **few documents**, its IDF will be high, meaning it has more significance for distinguishing between documents.\n",
    "\n",
    "### Why IDF is Important:\n",
    "IDF helps reduce the influence of common words like \"the,\" \"and,\" or \"is,\" which might appear frequently in all documents but provide little value for distinguishing between them. When combined with **TF (Term Frequency)**, it highlights words that are more informative for a given document.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "banana    0.405465\n",
       "appl      0.000000\n",
       "car       0.405465\n",
       "dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of documents (rows in TF_df)\n",
    "N = TF_df.shape[0]\n",
    "\n",
    "# Calculate document frequency (df) for each term (number of non-zero entries per column)\n",
    "df = (TF_df > 0).sum(axis=0)\n",
    "\n",
    "# Calculate the Inverse Document Frequency (IDF) using the formula\n",
    "IDF = np.log((1 + N) / (1 + df))\n",
    "IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "\n",
    "**TF-IDF** (Term Frequency-Inverse Document Frequency) is a measure that combines two metrics:\n",
    "- **TF (Term Frequency)**: Reflects how frequently a term appears in a document.\n",
    "- **IDF (Inverse Document Frequency)**: Reduces the weight of terms that appear in many documents, highlighting terms that are more unique to specific documents.\n",
    "\n",
    "The **TF-IDF score** is calculated by multiplying the TF value of a term by its IDF value:\n",
    "\n",
    "TF_IDF = TF*IDF\n",
    "\n",
    "\n",
    "This helps identify terms that are both frequent within a document and rare across the corpus, making them more important for understanding document content.\n",
    "\n",
    "\n",
    "A high weight in tf-idf is reached by a high term frequency (in the given document) and a low document frequency of the term in the whole collection of documents; the weights hence tend to filter out common terms. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>banana</th>\n",
       "      <th>appl</th>\n",
       "      <th>car</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.135155</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.27031</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     banana  appl      car\n",
       "0  0.135155   0.0  0.00000\n",
       "1  0.000000   0.0  0.27031"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the TF-IDF by multiplying TF values by the corresponding IDF values for each term\n",
    "TF_IDF_df = TF_df * IDF\n",
    "\n",
    "# Display the resulting TF-IDF DataFrame\n",
    "TF_IDF_df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Content",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
