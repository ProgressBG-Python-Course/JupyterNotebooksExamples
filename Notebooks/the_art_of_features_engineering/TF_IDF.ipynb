{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nemsys/projects/courses/ProgressBG/JupyterNotebooksExamples/.venv/lib/python3.7/site-packages/pandas/compat/__init__.py:85: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n",
      "/home/nemsys/projects/courses/ProgressBG/JupyterNotebooksExamples/.venv/lib/python3.7/site-packages/pandas/compat/__init__.py:85: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "docA = \"apple apple apple apple banana banana \"\n",
    "docB = \"apple car car\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Represent each document as 'Bag of words' (bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bowA = docA.split(\" \")\n",
    "bowB = docB.split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove empty strings, resulting from split\n",
    "bowA = list(filter( lambda w: w, bowA))\n",
    "bowB = list(filter( lambda w: w, bowB))\n",
    "\n",
    "# TASK: remove words which contain only 1 symbol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Bag of words for docA:\\n{bowA}')\n",
    "print(f'Bag of words for docB:\\n{bowB}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the collection vocabulary\n",
    "\n",
    "Make one list (set) of unique words for document collection\n",
    "\n",
    "That set will represent the vocabulary of our documents collection\n",
    "\n",
    "Note, in reality, we do not use \"words\", but rather-\"terms\", as each words is first stemmed.\n",
    "\n",
    "Another common preprocessing is to remove the 'stop words'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = set(bowA).union(set(bowB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count unique words in each document, i.e. Term Frequency\n",
    "\n",
    "Term frequency indicates the significance of a particular term in the document\n",
    "\n",
    "Now, we have to represent each document as numbers of terms occurrence in the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFa = dict.fromkeys(vocabulary, 0) \n",
    "TFb = dict.fromkeys(vocabulary, 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### using the fact, that keys are unique values:\n",
    "for word in bowA:\n",
    "    TFa[word]+=1\n",
    "    \n",
    "for word in bowB:\n",
    "    TFb[word]+=1\n",
    "    \n",
    "    \n",
    "### using Python count method:\n",
    "# countsA[word] = bowA.count(word)\n",
    "# countsB[word] = bowB.count(word)\n",
    "\n",
    "print(f'countsA: {TFa}')\n",
    "print(f'countsB: {TFb}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create DF to store document colection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_df = pd.DataFrame([TFa, TFb])\n",
    "counts_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate adjusted TF\n",
    "\n",
    "adjusted term frequency for document = counts_term / (number of terms in d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new df for the TF\n",
    "TF_df = counts_df.copy()\n",
    "\n",
    "for i, row in TF_df.iterrows(): \n",
    "    total = row.sum()            \n",
    "    TF_df.iloc[i] = row/total\n",
    "    print(f'total: {total}')\n",
    "     \n",
    "     \n",
    "TF_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute IDF\n",
    "\n",
    "The inverse document frequency is a measure of how much information the word provides.\n",
    "\n",
    "IDF(t) = log(Total number of documents / Number of documents with term i in it).\n",
    "\n",
    "\\begin{equation*}\n",
    "{IDF}(i, D) =  \\log \\frac{N}{Ni}\n",
    "\\end{equation*}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the IDF_df\n",
    "IDF_df = counts_df.copy()\n",
    "IDF_df\n",
    "\n",
    "N = len(IDF_df)\n",
    "print(f'Number of documents N = {(N)}')\n",
    "\n",
    "N_per_term = IDF_df.astype(bool).sum(axis=0)\n",
    "print(N_per_term)\n",
    "\n",
    "for i, row in counts_df.iterrows():     \n",
    "    IDF_df.iloc[i] = np.log(N/N_per_term)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDF_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "\n",
    "Term frequency–Inverse document frequency.\n",
    "\n",
    "A high weight in tf–idf is reached by a high term frequency (in the given document) and a low document frequency of the term in the whole collection of documents; the weights hence tend to filter out common terms. \n",
    "\n",
    "TF_IDF = TF*IDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF_IDF = counts_df.copy()\n",
    "TF_IDF = TF_df * IDF_df\n",
    "\n",
    "TF_IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Content",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
