{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn import tree,metrics\n",
    "\n",
    "from IPython.display import Image, display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    "\n",
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are decision trees\n",
    "\n",
    "Decision trees are a popular supervised learning algorithm used for both classification and regression tasks.\n",
    "\n",
    "They model decisions in the form of a tree, where each internal node represents a feature, each branch represents a decision rule, and each leaf represents an outcome.\n",
    "\n",
    "<a href=\"../images/from_data_to_decision_tree.bmp\"><img src=\"../images/from_data_to_decision_tree.bmp\" alt=\"from_data_to_decision_tree\" style=\"height: 18em;\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Characteristics of Decision Trees\n",
    "\n",
    "1. **Interpretability and Visualization:**\n",
    "   - They are easy to interpret and visualize, allowing for transparent decision-making processes and easier communication of results to non-technical stakeholders.\n",
    "\n",
    "2. **Applicability to Regression and Classification:**\n",
    "   - Decision trees can be applied to both regression and classification tasks:\n",
    "     - **Classification Trees:** Predict categorical class labels.\n",
    "     - **Regression Trees:** Predict continuous numerical values.\n",
    "\n",
    "3. **Handling of Continuous and Categorical Variables:**\n",
    "   - They can handle both continuous and categorical input features. Techniques like thresholding are used for continuous variables, and category grouping for categorical variables.\n",
    "\n",
    "4. **Hierarchical Structure and Divide-and-Conquer Strategy:**\n",
    "   - Decision trees are hierarchical data structures that implement a [divide-and-conquer](https://en.wiktionary.org/wiki/divide_and_conquer) strategy. They recursively partition the dataset into subsets based on attribute values to simplify complex problems.\n",
    "\n",
    "5. **Non-Parametric Model:**\n",
    "   - Decision trees are non-parametric, meaning they make no assumptions about the underlying distribution of the data.\n",
    "\n",
    "6. **Use of Information Theory:**\n",
    "   - They extensively utilize concepts from information theory, such as entropy and information gain, to determine the optimal attribute for splitting the data at each node.\n",
    "\n",
    "7. **Representation as If-Then Rules:**\n",
    "   - They can be regarded as a set of *if-then* rules. Each path from the root to a leaf node represents a conjunction (logical AND) of conditions that lead to a specific prediction.\n",
    "\n",
    "8. **Feature Importance Estimation:**\n",
    "   - They can provide estimates of feature importance, helping in feature selection and understanding the influence of different variables.\n",
    "\n",
    "9. **Prone to Overfitting:**\n",
    "   - They can easily overfit the training data if not properly pruned or regularized. Techniques like pruning, setting minimum sample leaf sizes, or limiting tree depth are used to mitigate this.\n",
    "\n",
    "10. **Ensemble Methods:**\n",
    "    - Decision trees serve as the foundational models for ensemble methods like Random Forests and Gradient Boosting Machines, which improve predictive performance by combining multiple trees.\n",
    "\n",
    "11. **Missing Value Handling:**\n",
    "    - Some decision tree algorithms can handle missing values by assigning probabilities to different paths or using surrogate splits.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Structure\n",
    "\n",
    "<a href=\"../images/basic_tree_terms.png\"><img src=\"../images/basic_tree_terms.png\" alt=\"\" style=\"height:14em\"></a>\n",
    "\n",
    "- **Each Node Tests an Attribute (Asks a Question):**\n",
    "  - The starting node is called the **root node**.\n",
    "  - Subsequent nodes that perform tests are called **internal nodes**.\n",
    "\n",
    "- **Each Branch Corresponds to an Attribute Value:**\n",
    "  - Branches represent the possible outcomes of a test and lead to the next node.\n",
    "\n",
    "- **Each Leaf Node Assigns a Classification:**\n",
    "  - Leaf nodes (terminal nodes) provide the final decision or prediction.\n",
    "  - No further splitting occurs at these points.\n",
    "\n",
    "- **Pure Node:**\n",
    "  - A node where all the samples have the same class label.\n",
    "  - Indicates complete homogeneity and requires no further splitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Decision Trees Work\n",
    "\n",
    "A decision tree splits the data based on certain conditions at each node.\n",
    "\n",
    "The goal is to create the best possible splits to maximize information gain (for classification) or minimize variance (for regression).\n",
    "\n",
    "### *Divide and Conquer* Algorithm\n",
    "\n",
    "1. **Divide the Dataset:**\n",
    "   - The algorithm selects the *best feature (attribute)* to split the dataset, using criteria like the **Gini Index** or **Information Gain**.\n",
    "   - The dataset is split into subsets based on the chosen feature's values.\n",
    "\n",
    "2. **Conquer by Recursive Splitting:**\n",
    "   - The algorithm repeats the process for each subset, selecting the best feature and splitting the data again.\n",
    "   - This continues until one of the stopping conditions is met, such as:\n",
    "     - The data in a subset being **pure** (all samples have the same class label).\n",
    "     - No features left to split.\n",
    "     - Reaching a predefined **depth limit**.\n",
    "   - When a stopping condition is met, a **leaf node** is created, representing the final class label or prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting the Best Feature to Split the Dataset\n",
    "\n",
    "In decision tree algorithms, selecting the optimal feature (attribute) to split the dataset at each node is crucial for building an effective model. The goal is to choose the feature that best separates the data into homogeneous subsets - subsets where the target variable is most uniform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy Introduction\n",
    "\n",
    "Entropy is a fundamental concept in information theory that measures the expected amount of information produced by a random process.  \n",
    "In the context of decision trees, entropy is used to determine the best way to split data to maximize information gain\n",
    "\n",
    "**Introduced by Claude Shannon in 1948:**\n",
    "  - In his seminal paper [\"A Mathematical Theory of Communication\"](https://en.wikipedia.org/wiki/A_Mathematical_Theory_of_Communication), Claude Shannon introduced the concept of entropy as a measure of uncertainty or information content in a communication system.\n",
    "\n",
    "**Measurement in Bits:**\n",
    "  - Entropy is typically measured in **bits** when the logarithm base 2 is used in its calculation.\n",
    "\n",
    "**Entropy Formula**\n",
    "\n",
    "  $$H(X) = - \\sum_{i=1}^{n} P(x_i) \\log_2 P(x_i)$$\n",
    "\n",
    "   Where:\n",
    "   - $H(X)$ is the entropy of the random variable X.\n",
    "   - $P(x_i)$ is the probability of outcome $x_i$.\n",
    "   - $n$ is the total number of possible outcomes.\n",
    "   - The logarithm is base 2 (for bits).\n",
    "\n",
    "**Example - Entropy of a Fair Coin Toss:**\n",
    "  - A fair coin has two possible outcomes: heads or tails, each with a probability of $0.5$.\n",
    "  - **Calculating the Entropy:**\n",
    "    $$\n",
    "    \\begin{align*}\n",
    "    E &= -\\left( p_{\\text{heads}} \\log_2 p_{\\text{heads}} + p_{\\text{tails}} \\log_2 p_{\\text{tails}} \\right) \\\\\n",
    "      &= -\\left( 0.5 \\times \\log_2 0.5 + 0.5 \\times \\log_2 0.5 \\right) \\\\\n",
    "      &= -\\left( 0.5 \\times (-1) + 0.5 \\times (-1) \\right) \\\\\n",
    "      &= -\\left( -0.5 - 0.5 \\right) \\\\\n",
    "      &= 1 \\text{ bit}\n",
    "    \\end{align*}\n",
    "    $$\n",
    "    - This means each toss of a fair coin carries **1 bit** of information.\n",
    "\n",
    "**Zero Entropy Situations:**\n",
    "  - Entropy is **zero** when an outcome is certain to occur.\n",
    "  - **Example:**\n",
    "    - If we know a coin will definitely land on heads (probability of heads is 1), then:\n",
    "      $$\n",
    "      E = -\\left( 1 \\times \\log_2 1 \\right) = -\\left( 1 \\times 0 \\right) = 0 \\text{ bits}\n",
    "      $$\n",
    "    - Similarly, a message like \"The sun will rise tomorrow\" carries **0 bits** of information because it's a certain event (assuming no uncertainty).\n",
    "\n",
    "<a href=\"../images/entropyfunction.png\"><img src=\"../images/entropyfunction.png\" style=\"height:14em\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information Gain: The reduction in entropy after a dataset is split on a feature.\n",
    "\n",
    "  - **Information Gain Formula:**\n",
    "    $$\n",
    "    IG(S, A) = E(S) - \\sum_{v \\in \\mathrm{Values}(A)} \\frac{|S_v|}{|S|} E(S_v)\n",
    "    $$\n",
    "    where:\n",
    "    - $IG(S, A)$ is the information gain of feature $A$.\n",
    "    - $S_v$ is the subset of $S$ where feature $A$ has value $v$.\n",
    "    - $|S_v|$ and $|S|$ are the number of samples in $S_v$ and $S$, respectively.\n",
    "    - $\\mathrm{Values}(A)$ is the set of all possible values of feature $A$.\n",
    "\n",
    "2. **Gini Impurity:**\n",
    "   - Measures the probability of incorrectly classifying a randomly chosen element if it was randomly labeled according to the distribution of labels in the subset.\n",
    "     - **Gini Impurity Formula:**\n",
    "       $$\n",
    "       G(S) = 1 - \\sum_{i=1}^{c} p_i^2\n",
    "       $$\n",
    "       where:\n",
    "       - $G(S)$ is the Gini impurity of dataset $S$.\n",
    "       - $p_i$ is the proportion of samples belonging to class $i$.\n",
    "\n",
    "   - **Gini Gain:** The reduction in Gini impurity after splitting, calculated similarly to information gain.\n",
    "\n",
    "3. **Reduction in Variance (For Regression Trees):**\n",
    "   - Used when the target variable is continuous.\n",
    "   - Splits are chosen to minimize the variance within subsets.\n",
    "     - **Variance Formula:**\n",
    "       $$\n",
    "       \\mathrm{Var}(S) = \\frac{1}{|S|} \\sum_{i=1}^{|S|} (y_i - \\bar{y})^2\n",
    "       $$\n",
    "       where:\n",
    "       - $\\mathrm{Var}(S)$ is the variance of the target variable in dataset $S$.\n",
    "       - $y_i$ is the target value of the $i^\\text{th}$ sample.\n",
    "       - $\\bar{y}$ is the mean of the target values in $S$.\n",
    "       - $|S|$ is the number of samples in $S$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process of Selecting the Best Feature:\n",
    "\n",
    "1. **Calculate the Impurity of the Entire Dataset:**\n",
    "   - Compute the initial impurity measure (entropy or Gini impurity) for the dataset before splitting.\n",
    "\n",
    "2. **Evaluate Each Feature:**\n",
    "   - For each feature:\n",
    "     - **For Categorical Features:**\n",
    "       - Split the dataset based on each possible value of the feature.\n",
    "     - **For Continuous Features:**\n",
    "       - Consider potential split points (thresholds) by sorting the values and evaluating splits at each threshold between adjacent values.\n",
    "\n",
    "3. **Compute the Gain for Each Feature:**\n",
    "   - Calculate the impurity of each subset resulting from the split.\n",
    "   - Compute the weighted average impurity after the split.\n",
    "   - Calculate the gain (reduction in impurity) achieved by the split:\n",
    "     $$\n",
    "     \\text{Gain} = \\text{Impurity before split} - \\text{Weighted impurity after split}\n",
    "     $$\n",
    "\n",
    "4. **Select the Feature with the Highest Gain:**\n",
    "   - Choose the feature (and threshold, if applicable) that results in the highest gain.\n",
    "   - This feature is used to split the dataset at the current node.\n",
    "\n",
    "### Considerations:\n",
    "\n",
    "- **Handling Overfitting:**\n",
    "  - Be cautious of features that create too many small subsets; this can lead to overfitting.\n",
    "  - Techniques like pruning and setting minimum samples per leaf help mitigate this risk.\n",
    "\n",
    "- **Computational Efficiency:**\n",
    "  - For datasets with many features or high cardinality, computing gains can be resource-intensive.\n",
    "  - Some algorithms use heuristics or random sampling to improve efficiency.\n",
    "\n",
    "- **Bias Towards Features with Many Levels:**\n",
    "  - Information gain can be biased towards features with many levels.\n",
    "  - **Information Gain Ratio** adjusts for this by normalizing the gain:\n",
    "    $$\n",
    "    \\text{Gain Ratio} = \\frac{\\text{Information Gain}}{\\text{Split Information}}\n",
    "    $$\n",
    "    where **Split Information** measures the potential information generated by splitting the dataset.\n",
    "\n",
    "### Summary:\n",
    "\n",
    "- The selection of the best feature to split on at each node is based on maximizing the reduction in impurity.\n",
    "- Common impurity measures include entropy (for information gain) and Gini impurity.\n",
    "- The algorithm evaluates all possible splits and selects the one that best partitions the data into homogeneous subsets.\n",
    "- This process is recursive and continues until stopping criteria are met (e.g., maximum depth, minimum samples per node, or pure nodes).\n",
    "\n",
    "By systematically selecting features that provide the most significant reduction in impurity, the decision tree algorithm builds a model that effectively predicts the target variable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DecisionTreeClassifier in scikit-learn\n",
    "\n",
    "The `DecisionTreeClassifier` is a class in the scikit-learn library used for classification tasks based on decision trees.\n",
    "\n",
    "It allows for easy implementation of decision tree models, offering control over the tree's depth, splitting criteria, and other hyperparameters.\n",
    "\n",
    "```python\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "    # Create a DecisionTreeClassifier object\n",
    "    clf = DecisionTreeClassifier(criterion='gini', max_depth=3)\n",
    "\n",
    "    # Fit the model to training data\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = clf.predict(X_test)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes on Feature Encoding \n",
    "\n",
    "Theoretically, for decision trees, it is not necessary to scale or normalize features, as the algorithm does not rely on the distance between data points.\n",
    "\n",
    "However, in scikit-learn the DecisionTreeClassifier requires categorical features to be encoded numerically. Ussually encoding is done with Label Encoding, because its effcient.\n",
    "\n",
    "Decision Trees, unlike linear models, do not assume any order or distance between the values of categorical features. For instance, in the case of Outlook with values \"Sunny\", \"Rain\", and \"Overcast\", even though we use numerical labels (e.g., 0, 1, 2), the tree will not interpret these as having a natural order. It will split the feature purely based on the information gain (or Gini/entropy), treating each value separately, just like it would for distinct categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pros and Cons of Decision Trees Algorithm\n",
    "\n",
    "### Pros:\n",
    "\n",
    "1. **Easily interpretable (not a \"Black-Box\")**  \n",
    "   Decision Trees are highly interpretable, allowing users to understand and explain how the model reaches any decision.\n",
    "\n",
    "1. **Requires minimal data preparation**  \n",
    "   Unlike many algorithms, Decision Trees require little to no data preprocessing, such as normalization or scaling.\n",
    "\n",
    "1. **Handles missing data and irrelevant attributes**  \n",
    "   Decision Trees can work effectively even with missing data or irrelevant features, where irrelevant attributes may have a gain of zero.\n",
    "\n",
    "1. **Can model problems with multiple outputs**  \n",
    "   Decision Trees can handle and predict multiple output variables simultaneously.\n",
    "\n",
    "1. **Low memory footprint**  \n",
    "   After pruning, Decision Trees are highly compact and require minimal memory.\n",
    "\n",
    "1. **Fast prediction phase (O(treeDepth))**  \n",
    "   While training may take time, prediction using Decision Trees is extremely fast, proportional to the depth of the tree.\n",
    "\n",
    "1. **Non-parametric**  \n",
    "   Decision Trees do not assume a predefined distribution for the data, making them suitable for various types of data distributions. More on [non-parametric models](https://machinelearningmastery.com/parametric-and-nonparametric-machine-learning-algorithms/).\n",
    "\n",
    "### Cons:\n",
    "\n",
    "1. **Prone to overfitting**  \n",
    "   Decision Trees can easily overfit the training data, especially if the tree is too deep, leading to poor generalization on new data.\n",
    "\n",
    "1. **Unstable (high variance)**  \n",
    "   Small changes in the data can result in a completely different tree structure, making Decision Trees sensitive to variations in the dataset.\n",
    "\n",
    "1. **Biased with imbalanced data**  \n",
    "   Decision Trees may favor classes with more instances, leading to biased predictions if the dataset is imbalanced.\n",
    "\n",
    "1. **Inefficient with large datasets**  \n",
    "   As the dataset size grows, Decision Trees can become computationally expensive to train, especially if the depth is not constrained.\n",
    "\n",
    "1. **Limited to axis-aligned splits**  \n",
    "   Decision Trees only make splits that are parallel to the feature axes, which may not capture more complex decision boundaries well.\n",
    "\n",
    "1. **Requires pruning for optimal performance**  \n",
    "   Without pruning, Decision Trees can grow very large, which negatively impacts performance and interpretability.\n",
    "\n",
    "1. **Poor extrapolation**  \n",
    "   Decision Trees are not good at extrapolating beyond the range of the training data, which can limit their predictive power in some cases.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applications of Decision Trees:\n",
    "\n",
    "1. **Classification tasks**  \n",
    "   Decision Trees are widely used for classifying objects, events, or people into predefined categories, such as spam detection, medical diagnosis, and customer segmentation.\n",
    "\n",
    "1. **Regression tasks**  \n",
    "   Decision Trees can be applied for predicting continuous values, such as house prices, stock market trends, and demand forecasting.\n",
    "\n",
    "1. **Feature selection**  \n",
    "   Decision Trees help identify the most important features in a dataset by calculating feature importance through information gain or Gini impurity.\n",
    "\n",
    "### Areas of application:\n",
    "\n",
    "1. **Credit scoring**  \n",
    "   Financial institutions use Decision Trees to evaluate the risk of loan applicants and predict whether a customer is likely to default.\n",
    "\n",
    "1. **Fraud detection**  \n",
    "   Decision Trees are employed in detecting fraudulent activities in transactions by learning patterns and anomalies in the data.\n",
    "\n",
    "1. **Customer churn prediction**  \n",
    "   Companies use Decision Trees to predict customer churn by analyzing customer behavior and identifying factors that lead to customer loss.\n",
    "\n",
    "1. **Medical diagnosis and decision making**  \n",
    "   In healthcare, Decision Trees assist in diagnosing diseases based on symptoms and test results, as well as making treatment recommendations.\n",
    "\n",
    "1. **Market basket analysis**  \n",
    "   Decision Trees help in understanding customer purchasing behavior by analyzing patterns in transactions, enabling targeted marketing strategies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: calculate Information Gain\n",
    "\n",
    "We will build a Decision Tree using the following dataset to predict whether we can \"Play\" based on the features **Outlook**, **Humidity**, and **Wind**.\n",
    "\n",
    "| Outlook  | Humidity | Wind   | Play |\n",
    "|----------|----------|--------|------|\n",
    "| Sunny    | High     | Weak   | No   |\n",
    "| Sunny    | High     | Strong | No   |\n",
    "| Overcast | High     | Weak   | Yes  |\n",
    "| Rain     | High     | Weak   | Yes  |\n",
    "| Rain     | Normal   | Weak   | Yes  |\n",
    "| Rain     | Normal   | Strong | No   |\n",
    "| Overcast | Normal   | Strong | Yes  |\n",
    "| Sunny    | High     | Weak   | No   |\n",
    "| Sunny    | Normal   | Weak   | Yes  |\n",
    "| Rain     | Normal   | Weak   | Yes  |\n",
    "| Sunny    | Normal   | Strong | Yes  |\n",
    "| Overcast | High     | Strong | Yes  |\n",
    "| Overcast | Normal   | Weak   | Yes  |\n",
    "| Rain     | High     | Strong | No   |\n",
    "\n",
    "\n",
    "1. **Calculate the entropy of the target (Play) variable:**\n",
    "   - There are 9 \"Yes\" and 5 \"No\" instances.\n",
    "   - The entropy is calculated as:\n",
    "\n",
    "   $$\n",
    "   H(Play) = - \\left( \\frac{9}{14} \\log_2 \\frac{9}{14} \\right) - \\left( \\frac{5}{14} \\log_2 \\frac{5}{14} \\right) = 0.940\n",
    "   $$\n",
    "\n",
    "2. **Calculate Information Gain for each feature:**\n",
    "   - We calculate the Information Gain for **Outlook**, **Humidity**, and **Wind** by splitting on each feature and calculating the weighted average entropy for the subsets.\n",
    "   \n",
    "   We split **Outlook** into three categories: Sunny, Overcast, and Rain. The entropy is calculated for each subset, and then we calculate the weighted average entropy for **Outlook**.\n",
    "\n",
    "   1. For **Sunny** (5 instances: 3 No, 2 Yes):\n",
    "\n",
    "      $$\n",
    "      H(Sunny) = - \\left( \\frac{3}{5} \\log_2 \\frac{3}{5} \\right) - \\left( \\frac{2}{5} \\log_2 \\frac{2}{5} \\right) = 0.971\n",
    "      $$\n",
    "\n",
    "   2. For **Overcast** (4 instances: 0 No, 4 Yes):\n",
    "\n",
    "      $$\n",
    "      H(Overcast) = - \\left( \\frac{4}{4} \\log_2 \\frac{4}{4} \\right) = 0\n",
    "      $$\n",
    "\n",
    "   3. For **Rain** (5 instances: 3 Yes, 2 No):\n",
    "\n",
    "      $$\n",
    "      H(Rain) = - \\left( \\frac{3}{5} \\log_2 \\frac{3}{5} \\right) - \\left( \\frac{2}{5} \\log_2 \\frac{2}{5} \\right) = 0.971\n",
    "      $$\n",
    "\n",
    "   Now, calculate the weighted average entropy for **Outlook**:\n",
    "\n",
    "      $$\n",
    "      H(Outlook) = \\frac{5}{14} \\cdot H(Sunny) + \\frac{4}{14} \\cdot H(Overcast) + \\frac{5}{14} \\cdot H(Rain)\n",
    "      $$\n",
    "\n",
    "   Substituting the values:\n",
    "\n",
    "      $$\n",
    "      H(Outlook) = \\frac{5}{14} \\cdot 0.971 + \\frac{4}{14} \\cdot 0 + \\frac{5}{14} \\cdot 0.971 = 0.693\n",
    "      $$\n",
    "\n",
    "   Thus, the entropy for **Outlook** is approximately 0.693.\n",
    "\n",
    "\n",
    "   - Information Gain for **Outlook**:\n",
    "\n",
    "   $$\n",
    "   IG(Outlook) = 0.940 - 0.693 = 0.247\n",
    "   $$\n",
    "\n",
    "   **Humidity**:\n",
    "   - Split into High (3 Yes, 4 No), Normal (6 Yes, 1 No).\n",
    "   - The entropy for **Humidity** is:\n",
    "\n",
    "   $$\n",
    "   H(Humidity) = \\frac{7}{14}(0.985) + \\frac{7}{14}(0.592) = 0.788\n",
    "   $$\n",
    "\n",
    "   - Information Gain for **Humidity**:\n",
    "\n",
    "   $$\n",
    "   IG(Humidity) = 0.940 - 0.788 = 0.152\n",
    "   $$\n",
    "\n",
    "   **Wind**:\n",
    "   - Split into Weak (6 Yes, 2 No), Strong (3 Yes, 3 No).\n",
    "   - The entropy for **Wind** is:\n",
    "\n",
    "   $$\n",
    "   H(Wind) = \\frac{8}{14}(0.811) + \\frac{6}{14}(1) = 0.892\n",
    "   $$\n",
    "\n",
    "   - Information Gain for **Wind**:\n",
    "\n",
    "   $$\n",
    "   IG(Wind) = 0.940 - 0.892 = 0.048\n",
    "   $$\n",
    "\n",
    "3. **Choose the feature with the highest Information Gain:**\n",
    "   - **Outlook** has the highest information gain (0.247), so we split on **Outlook**.\n",
    "\n",
    "4. **Split the data based on Outlook:**\n",
    "   - For **Overcast**, all instances are \"Yes\", so this becomes a leaf node labeled \"Yes\".\n",
    "   - For **Rain** and **Sunny**, further splits are needed.\n",
    "\n",
    "5. **Repeat the process for the remaining splits** using **Humidity** and **Wind** to build the complete tree.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hands On: Will John play Tennis?\n",
    " \n",
    "### Data insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Outlook</th>\n",
       "      <th>Humidity</th>\n",
       "      <th>Wind</th>\n",
       "      <th>Play</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Overcast</td>\n",
       "      <td>High</td>\n",
       "      <td>Strong</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Overcast</td>\n",
       "      <td>High</td>\n",
       "      <td>Weak</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Overcast</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Strong</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Overcast</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Weak</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Rain</td>\n",
       "      <td>High</td>\n",
       "      <td>Strong</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Outlook Humidity    Wind Play\n",
       "11  Overcast     High  Strong  Yes\n",
       "2   Overcast     High    Weak  Yes\n",
       "6   Overcast   Normal  Strong  Yes\n",
       "12  Overcast   Normal    Weak  Yes\n",
       "13      Rain     High  Strong   No"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./play_tennis_data.csv')\n",
    "df.sort_values(by=['Outlook','Humidity','Wind'], inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_f757f_row0_col0, #T_f757f_row0_col1, #T_f757f_row0_col2, #T_f757f_row0_col3, #T_f757f_row1_col0, #T_f757f_row1_col1, #T_f757f_row1_col2, #T_f757f_row1_col3, #T_f757f_row2_col0, #T_f757f_row2_col1, #T_f757f_row2_col2, #T_f757f_row2_col3, #T_f757f_row3_col0, #T_f757f_row3_col1, #T_f757f_row3_col2, #T_f757f_row3_col3, #T_f757f_row5_col0, #T_f757f_row5_col1, #T_f757f_row5_col2, #T_f757f_row5_col3, #T_f757f_row7_col0, #T_f757f_row7_col1, #T_f757f_row7_col2, #T_f757f_row7_col3, #T_f757f_row8_col0, #T_f757f_row8_col1, #T_f757f_row8_col2, #T_f757f_row8_col3, #T_f757f_row12_col0, #T_f757f_row12_col1, #T_f757f_row12_col2, #T_f757f_row12_col3, #T_f757f_row13_col0, #T_f757f_row13_col1, #T_f757f_row13_col2, #T_f757f_row13_col3 {\n",
       "  color: green;\n",
       "}\n",
       "#T_f757f_row4_col0, #T_f757f_row4_col1, #T_f757f_row4_col2, #T_f757f_row4_col3, #T_f757f_row6_col0, #T_f757f_row6_col1, #T_f757f_row6_col2, #T_f757f_row6_col3, #T_f757f_row9_col0, #T_f757f_row9_col1, #T_f757f_row9_col2, #T_f757f_row9_col3, #T_f757f_row10_col0, #T_f757f_row10_col1, #T_f757f_row10_col2, #T_f757f_row10_col3, #T_f757f_row11_col0, #T_f757f_row11_col1, #T_f757f_row11_col2, #T_f757f_row11_col3 {\n",
       "  color: red;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_f757f\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_f757f_level0_col0\" class=\"col_heading level0 col0\" >Outlook</th>\n",
       "      <th id=\"T_f757f_level0_col1\" class=\"col_heading level0 col1\" >Humidity</th>\n",
       "      <th id=\"T_f757f_level0_col2\" class=\"col_heading level0 col2\" >Wind</th>\n",
       "      <th id=\"T_f757f_level0_col3\" class=\"col_heading level0 col3\" >Play</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_f757f_level0_row0\" class=\"row_heading level0 row0\" >11</th>\n",
       "      <td id=\"T_f757f_row0_col0\" class=\"data row0 col0\" >Overcast</td>\n",
       "      <td id=\"T_f757f_row0_col1\" class=\"data row0 col1\" >High</td>\n",
       "      <td id=\"T_f757f_row0_col2\" class=\"data row0 col2\" >Strong</td>\n",
       "      <td id=\"T_f757f_row0_col3\" class=\"data row0 col3\" >Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f757f_level0_row1\" class=\"row_heading level0 row1\" >2</th>\n",
       "      <td id=\"T_f757f_row1_col0\" class=\"data row1 col0\" >Overcast</td>\n",
       "      <td id=\"T_f757f_row1_col1\" class=\"data row1 col1\" >High</td>\n",
       "      <td id=\"T_f757f_row1_col2\" class=\"data row1 col2\" >Weak</td>\n",
       "      <td id=\"T_f757f_row1_col3\" class=\"data row1 col3\" >Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f757f_level0_row2\" class=\"row_heading level0 row2\" >6</th>\n",
       "      <td id=\"T_f757f_row2_col0\" class=\"data row2 col0\" >Overcast</td>\n",
       "      <td id=\"T_f757f_row2_col1\" class=\"data row2 col1\" >Normal</td>\n",
       "      <td id=\"T_f757f_row2_col2\" class=\"data row2 col2\" >Strong</td>\n",
       "      <td id=\"T_f757f_row2_col3\" class=\"data row2 col3\" >Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f757f_level0_row3\" class=\"row_heading level0 row3\" >12</th>\n",
       "      <td id=\"T_f757f_row3_col0\" class=\"data row3 col0\" >Overcast</td>\n",
       "      <td id=\"T_f757f_row3_col1\" class=\"data row3 col1\" >Normal</td>\n",
       "      <td id=\"T_f757f_row3_col2\" class=\"data row3 col2\" >Weak</td>\n",
       "      <td id=\"T_f757f_row3_col3\" class=\"data row3 col3\" >Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f757f_level0_row4\" class=\"row_heading level0 row4\" >13</th>\n",
       "      <td id=\"T_f757f_row4_col0\" class=\"data row4 col0\" >Rain</td>\n",
       "      <td id=\"T_f757f_row4_col1\" class=\"data row4 col1\" >High</td>\n",
       "      <td id=\"T_f757f_row4_col2\" class=\"data row4 col2\" >Strong</td>\n",
       "      <td id=\"T_f757f_row4_col3\" class=\"data row4 col3\" >No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f757f_level0_row5\" class=\"row_heading level0 row5\" >3</th>\n",
       "      <td id=\"T_f757f_row5_col0\" class=\"data row5 col0\" >Rain</td>\n",
       "      <td id=\"T_f757f_row5_col1\" class=\"data row5 col1\" >High</td>\n",
       "      <td id=\"T_f757f_row5_col2\" class=\"data row5 col2\" >Weak</td>\n",
       "      <td id=\"T_f757f_row5_col3\" class=\"data row5 col3\" >Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f757f_level0_row6\" class=\"row_heading level0 row6\" >5</th>\n",
       "      <td id=\"T_f757f_row6_col0\" class=\"data row6 col0\" >Rain</td>\n",
       "      <td id=\"T_f757f_row6_col1\" class=\"data row6 col1\" >Normal</td>\n",
       "      <td id=\"T_f757f_row6_col2\" class=\"data row6 col2\" >Strong</td>\n",
       "      <td id=\"T_f757f_row6_col3\" class=\"data row6 col3\" >No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f757f_level0_row7\" class=\"row_heading level0 row7\" >4</th>\n",
       "      <td id=\"T_f757f_row7_col0\" class=\"data row7 col0\" >Rain</td>\n",
       "      <td id=\"T_f757f_row7_col1\" class=\"data row7 col1\" >Normal</td>\n",
       "      <td id=\"T_f757f_row7_col2\" class=\"data row7 col2\" >Weak</td>\n",
       "      <td id=\"T_f757f_row7_col3\" class=\"data row7 col3\" >Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f757f_level0_row8\" class=\"row_heading level0 row8\" >9</th>\n",
       "      <td id=\"T_f757f_row8_col0\" class=\"data row8 col0\" >Rain</td>\n",
       "      <td id=\"T_f757f_row8_col1\" class=\"data row8 col1\" >Normal</td>\n",
       "      <td id=\"T_f757f_row8_col2\" class=\"data row8 col2\" >Weak</td>\n",
       "      <td id=\"T_f757f_row8_col3\" class=\"data row8 col3\" >Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f757f_level0_row9\" class=\"row_heading level0 row9\" >1</th>\n",
       "      <td id=\"T_f757f_row9_col0\" class=\"data row9 col0\" >Sunny</td>\n",
       "      <td id=\"T_f757f_row9_col1\" class=\"data row9 col1\" >High</td>\n",
       "      <td id=\"T_f757f_row9_col2\" class=\"data row9 col2\" >Strong</td>\n",
       "      <td id=\"T_f757f_row9_col3\" class=\"data row9 col3\" >No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f757f_level0_row10\" class=\"row_heading level0 row10\" >0</th>\n",
       "      <td id=\"T_f757f_row10_col0\" class=\"data row10 col0\" >Sunny</td>\n",
       "      <td id=\"T_f757f_row10_col1\" class=\"data row10 col1\" >High</td>\n",
       "      <td id=\"T_f757f_row10_col2\" class=\"data row10 col2\" >Weak</td>\n",
       "      <td id=\"T_f757f_row10_col3\" class=\"data row10 col3\" >No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f757f_level0_row11\" class=\"row_heading level0 row11\" >7</th>\n",
       "      <td id=\"T_f757f_row11_col0\" class=\"data row11 col0\" >Sunny</td>\n",
       "      <td id=\"T_f757f_row11_col1\" class=\"data row11 col1\" >High</td>\n",
       "      <td id=\"T_f757f_row11_col2\" class=\"data row11 col2\" >Weak</td>\n",
       "      <td id=\"T_f757f_row11_col3\" class=\"data row11 col3\" >No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f757f_level0_row12\" class=\"row_heading level0 row12\" >10</th>\n",
       "      <td id=\"T_f757f_row12_col0\" class=\"data row12 col0\" >Sunny</td>\n",
       "      <td id=\"T_f757f_row12_col1\" class=\"data row12 col1\" >Normal</td>\n",
       "      <td id=\"T_f757f_row12_col2\" class=\"data row12 col2\" >Strong</td>\n",
       "      <td id=\"T_f757f_row12_col3\" class=\"data row12 col3\" >Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f757f_level0_row13\" class=\"row_heading level0 row13\" >8</th>\n",
       "      <td id=\"T_f757f_row13_col0\" class=\"data row13 col0\" >Sunny</td>\n",
       "      <td id=\"T_f757f_row13_col1\" class=\"data row13 col1\" >Normal</td>\n",
       "      <td id=\"T_f757f_row13_col2\" class=\"data row13 col2\" >Weak</td>\n",
       "      <td id=\"T_f757f_row13_col3\" class=\"data row13 col3\" >Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fb356319b20>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def highlight_play(row):\n",
    "    '''Function to apply color based on the value in the 'Play' column'''\n",
    "    color = 'red' if row['Play'] == 'No' else 'green'\n",
    "    return [f'color:{color}'] * len(row)\n",
    "\n",
    "# Show data with colors\n",
    "df.style.apply(highlight_play, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data\n",
    "\n",
    "#### Categorical attributes to values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split to train/test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viualize decision bounaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# visualize the model's decision regions\n",
    "from utils import plot_decision\n",
    "\n",
    "X_combined = np.vstack((X_train, X_test))\n",
    "y_combined = np.hstack((y_train, y_test))\n",
    "plot_decision(X=X_combined, y=y_combined, classifier=fitted)\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Content",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
