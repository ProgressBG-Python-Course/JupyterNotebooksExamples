{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifier explained by simple example (Spam/Ham Mails)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Problem:\n",
    "\n",
    "Imagine we want to classify an email as \"Spam\" or \"Ham\" (not spam) based on certain words it contains. We will use a simple Naive Bayes classifier to do this, which assumes that the presence of each word is independent of the others.\n",
    "\n",
    "We have the following dataset of emails, where each email is labeled as either \"Spam\" or \"Ham\" (not spam). Each email is represented by three features: whether it contains the words \"free\", \"offer\", and \"meeting\".\n",
    "\n",
    "| Email   | Contains \"free\" | Contains \"offer\" | Contains \"meeting\" | Class      |\n",
    "|---------|-----------------|------------------|---------------------|------------|\n",
    "| Email 1 | Yes             | Yes              | No                  | Spam       |\n",
    "| Email 2 | Yes             | No               | Yes                 | Spam       |\n",
    "| Email 5 | Yes             | No               | No                  | Spam       |\n",
    "| Email 3 | No              | Yes              | No                  | Ham        |\n",
    "| Email 4 | No              | No               | Yes                 | Ham        |\n",
    "\n",
    "Our goal is to classify a new email that contains the words **\"free\"** and **\"offer\"**, but **not \"meeting\"** as \"Spam\" or \"Ham\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution by hand\n",
    "\n",
    "Based on the dataset, we'll calculate the probabilities for the new email being \"Spam\" or \"Ham\" and choose the class with the highest probability. We will use the Naive Bayes formula, which takes into account the prior probabilities of each class and the likelihood of the features given each class.\n",
    "\n",
    "$$ \\hat y = \\arg \\mathop {\\max }\\limits_y \\prod\\nolimits_{i = 1}^n {{\\rm{P}}\\left( {{x_i}|y} \\right) {\\rm{P}}\\left( y \\right)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Calculate Prior Probabilities: $P(y)$\n",
    "\n",
    "The prior probability is the likelihood of a class occurring in the dataset. We calculate the prior probabilities for both **Spam** and **Ham**:\n",
    "\n",
    "- $P(\\text{Spam}) = \\frac{\\text{Number of Spam emails}}{\\text{Total emails}} = \\frac{3}{5} = 0.6$\n",
    "- $P(\\text{Ham}) = \\frac{\\text{Number of Ham emails}}{\\text{Total emails}} = \\frac{2}{5} = 0.4$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Calculate Likelihoods: $P(x_i | y)$\n",
    "\n",
    "The likelihoods are the conditional probabilities of the features (words) given the class.\n",
    "\n",
    "| Feature    | `P(Feature\\|Spam)`   | `P(Feature\\|Ham)`    |\n",
    "|------------|---------------------|---------------------|\n",
    "| \"free\"     | 3/3 = 1.0           | 0/2 = 0.0           |\n",
    "| \"offer\"    | 1/3 = 0.33          | 1/2 = 0.5           |\n",
    "| \"meeting\"  | 1/3 = 0.33          | 1/2 = 0.5           |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Apply Naive Bayes Formula\n",
    "\n",
    "Now, we classify a new email that contains the words **\"free\"** and **\"offer\"**.\n",
    "\n",
    "**Note**: We will take into account the absence of the word **\"meeting\"** (not_meeting), which is common in classifiers like Naive Bayes (including scikit-learn). However, there is another possible approach where the absence of a word could be ignored, focusing only on the features that are present. This approach might slightly affect the final probability, but the prediction outcome usually remains the same.\n",
    "\n",
    "For **Spam**:\n",
    "$$\n",
    "P(Spam | free, offer, not\\_meeting) \\propto P(free | Spam) * P(offer | Spam) * P(not\\_meeting | Spam) * P(Spam)\n",
    "$$\n",
    "\n",
    "Substitute the values:\n",
    "$$\n",
    "P(Spam | free, offer, not\\_meeting) \\propto 1.0 * 0.33 * (1-0.33) * 0.6 = 0.13266\n",
    "$$\n",
    "\n",
    "For **Ham**:\n",
    "$$\n",
    "P(Ham | free, offer, not\\_meeting) \\propto P(free | Ham) * P(offer | Ham) * P(not\\_meeting | Ham) * P(Ham)\n",
    "$$\n",
    "Substitute the values:\n",
    "$$\n",
    "P(Ham | free, offer, not\\_meeting) \\propto 0.0 * 0.5 * (1-0.5) * 0.4 = 0\n",
    "$$\n",
    "\n",
    "Current Result:\n",
    "- $P(Spam) = 0.13266$\n",
    "- $P(Ham) = 0$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Zero-probability problem\n",
    "\n",
    "We see that if we have any feature that has a probability of 0 (such as the word \"free\" in a \"Ham\" email), this will cause the entire class probability to become 0 when multiplied. I.e any mail containing 'free' will be classified as Spam.\n",
    "\n",
    "Common solution is the Laplace Smoothing, which avoid the problem by adding a constant (usually 1) to each feature's count. This ensures that no zero probabilities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 Revised: Calculate Likelihoods: $P(x_i | y)$ with Laplace Smoothing\n",
    "\n",
    "With Laplace Smoothing, we add 1 to the count of each word and increase the denominator by the number of possible outcomes (2, for presence/absence of words). This ensures no probability is zero.\n",
    "\n",
    "| Feature    | `P(Feature \\| Spam)`                |   `P(Feature \\| Ham)`               |\n",
    "|------------|-------------------------------------|-------------------------------------|\n",
    "| \"free\"     | $\\frac{3+1}{3+2} = \\frac{4}{5} = 0.8$   | $\\frac{0+1}{2+2} = \\frac{1}{4} = 0.25$ |\n",
    "| \"offer\"    | $\\frac{1+1}{3+2} = \\frac{3}{5} = 0.4$   | $\\frac{1+1}{2+2} = \\frac{2}{4} = 0.5$  |\n",
    "| \"meeting\"  | $\\frac{1+1}{3+2} = \\frac{2}{5} = 0.4$   | $\\frac{1+1}{2+2} = \\frac{2}{4} = 0.5$  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 Revised: Apply Naive Bayes Formula\n",
    "\n",
    "For **Spam**:\n",
    "$$\n",
    "P(Spam | free, offer, not meeting) \\propto P(free | Spam) * P(offer | Spam) * P(not meeting | Spam) * P(Spam)\n",
    "$$\n",
    "Substitute the values with smoothing:\n",
    "$$\n",
    "P(Spam | free, offer, not meeting) \\propto 0.8 * 0.4 * (1 - 0.4) * 0.6 = 0.1152\n",
    "$$\n",
    "\n",
    "For **Ham**:\n",
    "$$\n",
    "P(Ham | free, offer, not meeting) \\propto P(free | Ham) * P(offer | Ham) * P(not meeting | Ham) * P(Ham)\n",
    "$$\n",
    "Substitute the values with smoothing:\n",
    "$$\n",
    "P(Ham | free, offer, not meeting) \\propto 0.25 * 0.5 * (1 - 0.5) * 0.4 = 0.025\n",
    "$$\n",
    "\n",
    "Current Result:\n",
    "- $P(Spam) = 0.1152$\n",
    "- $P(Ham) = 0.025$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Normalize Probabilities\n",
    "\n",
    " We need to normalize probabilities results to ensure that the sum of the posterior probabilities for all classes equals 1, which allows for direct comparison between the classes.\n",
    "\n",
    " This is done by dividing each class's probability by the sum of the unnormalized probabilities.\n",
    "\n",
    " For **Spam**:\n",
    "\n",
    " $$\n",
    " P(\\text{Spam}) = \\frac{0.1152}{0.1152 + 0.025} = 0,82168331\n",
    " $$\n",
    "\n",
    " For **Ham**:\n",
    "\n",
    " $$\n",
    " P(\\text{Ham}) = \\frac{0.025}{0.1152 + 0.025} = 0,17831669\n",
    " $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Conclusion\n",
    "\n",
    "$P(Spam) \\approx 82.17\\%$, \n",
    "\n",
    "$P(Ham) \\approx 17.83\\%$\n",
    "\n",
    "\n",
    "The classifier predicts the new email as **Spam**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution with [scikit-learn Bernoulli Naive Bayes classifier](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted class: [1]\n",
      "probabilities for each class: $[[0.17831669 0.82168331]]\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Likelihoods (Spam first, then Ham):\n",
      "[[0.25 0.5  0.5 ]\n",
      " [0.8  0.4  0.4 ]]\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Feature counts (Spam first, then Ham):\n",
      "[[0. 1. 1.]\n",
      " [3. 1. 1.]]\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Vocabulary: {'free': 0, 'offer': 2, 'meeting': 1}\n",
      "Vectorised emails:\n",
      "[[1 0 1]\n",
      " [1 1 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "import numpy as np\n",
    "\n",
    "# Define the dataset (emails and their corresponding class: Spam or Ham)\n",
    "emails = [\n",
    "    'free offer',       # spam\n",
    "    'free meeting',     # spam\n",
    "    'free',             # spam\n",
    "    'offer',            # ham\n",
    "    'meeting',          # ham\n",
    "]\n",
    "\n",
    "# Corresponding labels: 1 for spam, 0 for ham\n",
    "labels = [1, 1, 1, 0, 0]\n",
    "\n",
    "# Vectorize the data to create a bag-of-words model (binary: presence/absence of words)\n",
    "vectorizer = CountVectorizer(binary=True)\n",
    "X = vectorizer.fit_transform(emails)\n",
    "\n",
    "# Initialize the Bernoulli Naive Bayes classifier\n",
    "bnb = BernoulliNB(alpha=1)\n",
    "\n",
    "# Train the classifier\n",
    "bnb.fit(X, labels)\n",
    "\n",
    "# Test it on a new email containing 'free' and 'offer', but not 'meeting'\n",
    "new_email = ['free offer']\n",
    "X_test = vectorizer.transform(new_email)\n",
    "\n",
    "# Predict the class for the new email\n",
    "predicted_class = bnb.predict(X_test)\n",
    "\n",
    "# Print predicted class and probabilities for each class\n",
    "print(f'predicted class: {predicted_class}')\n",
    "print(f'probabilities for each class: ${bnb.predict_proba(X_test)}')\n",
    "print('~'*80)\n",
    "\n",
    "# Print the likelihood probabilities for each class (Spam, Ham)\n",
    "log_likelihoods = bnb.feature_log_prob_\n",
    "likelihoods = np.exp(log_likelihoods) # Convert log probabilities to actual probabilities\n",
    "print(f'Likelihoods (Spam first, then Ham):\\n{likelihoods}')\n",
    "print('~'*80)\n",
    "\n",
    "# Print feature counts for each class (Spam and Ham)\n",
    "feature_counts = bnb.feature_count_\n",
    "print(f'Feature counts (Spam first, then Ham):\\n{feature_counts}')\n",
    "print('~'*80)\n",
    "\n",
    "# Print CountVectorizer details:\n",
    "print(f'Vocabulary: {vectorizer.vocabulary_}')\n",
    "print(f'Vectorised emails:\\n{X.toarray()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "The Bernoulli Naive Bayes classifier in scikit-learn has predicted that the new email containing the words \"free\" and \"offer\" (but not \"meeting\") is classified as Spam (class 1).\n",
    "\n",
    "The predicted probabilities are:\n",
    "\n",
    "$Ham (class 0) \\approx 17.83\\%$\n",
    "\n",
    "$Spam (class 1) \\approx 82.17\\%$, \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
