{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis on customer reviews\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Sentiment analysis is a technique used to determine the emotional tone of a piece of text. By performing sentiment analysis on customer reviews, we can gain insights into how customers feel about a product or service.\n",
    "\n",
    "\n",
    "Required Libraries\n",
    "------------------\n",
    "\n",
    "We will be using the following libraries in our project:\n",
    "\n",
    "*   `pandas`: Used for data manipulation and analysis\n",
    "*   `numpy`: Used for numerical operations\n",
    "*   `nltk`: Used for natural language processing tasks like stemming, lemmatization, and stopword removal\n",
    "*   `re`: Used for regular expressions\n",
    "*   `string`: Used for string operations\n",
    "*   `sklearn`: Used for building machine learning models\n",
    "\n",
    "Before we start with the project, let's install the required libraries using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pandas numpy nltk scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "For this project, we will be using the Amazon Fine Food Reviews dataset, which can be downloaded from Kaggle ([Amazon Fine Food Reviews](https://www.kaggle.com/datasets/snap/amazon-fine-food-reviews?select=Reviews.csv)). This dataset consists of reviews of fine foods from Amazon. The data spans from October 1999 to October 2012 and includes over 500,000 reviews.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(999, 10)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('Reviews.csv')\n",
    "\n",
    "### Read only 1000 rows, randomly\n",
    "# Set a random seed for reproducibility\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "# Get the number of rows in the CSV file\n",
    "num_rows = sum(1 for line in open('Reviews.csv'))\n",
    "\n",
    "# Define the number of rows to read\n",
    "nrows = 1000\n",
    "\n",
    "# Define the indices of the rows to skip\n",
    "skiprows = sorted(random.sample(range(1, num_rows), num_rows - nrows))\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('Reviews.csv', nrows=nrows, skiprows=skiprows)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove some random rows if memory error is raised\n",
    "# rows_to_remove = df.sample(frac=0.9)\n",
    "\n",
    "# # Use the drop function to remove those rows from the original dataframe\n",
    "# df = df.drop(rows_to_remove.index)\n",
    "# df.head()\n",
    "# df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Id', 'ProductId', 'UserId', 'ProfileName', 'HelpfulnessNumerator',\n",
       "       'HelpfulnessDenominator', 'Score', 'Time', 'Summary', 'Text'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Step 1: Data Cleaning\n",
    "---------------------\n",
    "\n",
    "The first step in our project is to clean the data. We will remove any unnecessary columns and rows, and perform text preprocessing on the reviews. Text preprocessing involves converting the text to lowercase, removing special characters, and removing stopwords.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/nemsys/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>BUY ELSEWHERE</td>\n",
       "      <td>find 6pack 40 dollars less diaperscom reason c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>Excellent coffee!</td>\n",
       "      <td>could get used nice strong robust coffee witho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>yumm!!</td>\n",
       "      <td>nice creamy usually dont justify buying kcups ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>Great product</td>\n",
       "      <td>brand lowest price find organic dog kibbles su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>deceptive photo</td>\n",
       "      <td>felt ripped photo clearly shows two bones pric...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Score            Summary                                               Text\n",
       "0      5      BUY ELSEWHERE  find 6pack 40 dollars less diaperscom reason c...\n",
       "1      5  Excellent coffee!  could get used nice strong robust coffee witho...\n",
       "2      5             yumm!!  nice creamy usually dont justify buying kcups ...\n",
       "3      5      Great product  brand lowest price find organic dog kibbles su...\n",
       "4      1    deceptive photo  felt ripped photo clearly shows two bones pric..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Drop any rows with missing values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# # Remove any unnecessary columns\n",
    "df.drop(['Id', 'ProductId', 'UserId', 'ProfileName', 'HelpfulnessNumerator', 'HelpfulnessDenominator', 'Time'], axis=1, inplace=True)\n",
    "\n",
    "# Convert the text to lowercase\n",
    "df['Text'] = df['Text'].str.lower()\n",
    "\n",
    "# Remove any special characters\n",
    "df['Text'] = df['Text'].apply(lambda x: re.sub('[^a-zA-z0-9\\s]', '', x))\n",
    "\n",
    "# Remove stopwords\n",
    "stopwords = set(stopwords.words('english'))\n",
    "df['Text'] = df['Text'].apply(lambda x: ' '.join([word for word in x.split() if word not in stopwords]))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Step 2: Feature Engineering\n",
    "---------------------------\n",
    "\n",
    "The next step is to extract features from the preprocessed text. We will use the Bag of Words model to extract features. In this model, we represent each review as a vector of word counts using the [Scikit-learn CountVectorizer()](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory for sparse array: 270384\n",
      "Memory for dense array: 58988952\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import scipy.sparse as sp\n",
    "\n",
    "\n",
    "# Create a CountVectorizer object\n",
    "cv = CountVectorizer()\n",
    "\n",
    "# Convert the text data to a sparse array\n",
    "X = cv.fit_transform(df['Text'])\n",
    "print(f'Memory for sparse array: {X.data.nbytes}')\n",
    "\n",
    "# Convert the sparse array to a dense array if you have enough RAM\n",
    "X = X.toarray()\n",
    "print(f'Memory for dense array: {X.data.nbytes}')\n",
    "\n",
    "# X = cv.fit_transform(df['Text']).toarray()\n",
    "y = df['Score'].values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(999, 7381)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse Array vs Dense Array\n",
    "\n",
    "A dense array is an array in which most of the elements have a value, and these values are typically non-zero. In other words, a dense array has very few empty or \"null\" elements, and most of the array cells are occupied with values. For example, an array containing the numbers: [1, 2, 3, 4, 5] would be considered a dense array.\n",
    "\n",
    "On the other hand, a sparse array is an array in which most of the elements have a value of zero or are empty. In other words, a sparse array has a lot of empty or \"null\" elements, and very few cells are occupied with values. For example, an array containing the numbers: [1, 0, 0, 4, 0] would be considered a sparse array.\n",
    "\n",
    "Sparse arrays can be more efficient in terms of memory usage and processing time when dealing with large datasets that have a lot of missing or zero values. This is because sparse arrays only store non-zero elements, which can save a significant amount of memory compared to dense arrays that store all elements regardless of their value. However, dense arrays are often faster to process because they have a smaller number of zero checks when compared to sparse arrays."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Building Machine Learning Models\n",
    "\n",
    "\n",
    "The final step is to build machine learning models to predict the sentiment of the reviews. We will use the following algorithms:\n",
    "\n",
    "*   Logistic Regression\n",
    "*   Naive Bayes\n",
    "*   Support Vector Machines\n",
    "*   Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.6166666666666667\n",
      "Logistic Regression Confusion Matrix: [[  2   2   1   4  12]\n",
      " [  2   2   0   4  11]\n",
      " [  1   0   1   3  10]\n",
      " [  1   0   1   3  40]\n",
      " [  3   5   3  12 177]]\n",
      "Logistic Regression Classification Report:               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.22      0.10      0.13        21\n",
      "           2       0.22      0.11      0.14        19\n",
      "           3       0.17      0.07      0.10        15\n",
      "           4       0.12      0.07      0.08        45\n",
      "           5       0.71      0.89      0.79       200\n",
      "\n",
      "    accuracy                           0.62       300\n",
      "   macro avg       0.29      0.24      0.25       300\n",
      "weighted avg       0.53      0.62      0.56       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred_lr = lr.predict(X_test)\n",
    "\n",
    "# Print the accuracy, confusion matrix, and classification report\n",
    "print('Logistic Regression Accuracy:', accuracy_score(y_test, y_pred_lr))\n",
    "print('Logistic Regression Confusion Matrix:', confusion_matrix(y_test, y_pred_lr))\n",
    "print('Logistic Regression Classification Report:', classification_report(y_test, y_pred_lr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy:\n",
      "0.6466666666666666\n",
      "\n",
      "Naive Bayes Confusion Matrix:\n",
      "[[  2   0   0   4  15]\n",
      " [  1   0   0   2  16]\n",
      " [  0   0   0   2  13]\n",
      " [  0   0   3   1  41]\n",
      " [  2   0   1   6 191]]\n",
      "\n",
      "Naive Bayes Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.40      0.10      0.15        21\n",
      "           2       1.00      0.00      0.00        19\n",
      "           3       0.00      0.00      0.00        15\n",
      "           4       0.07      0.02      0.03        45\n",
      "           5       0.69      0.95      0.80       200\n",
      "\n",
      "    accuracy                           0.65       300\n",
      "   macro avg       0.43      0.21      0.20       300\n",
      "weighted avg       0.56      0.65      0.55       300\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train, y_train)\n",
    "y_pred_nb = nb.predict(X_test)\n",
    "\n",
    "# Print the accuracy, confusion matrix, and classification report\n",
    "print(f'Naive Bayes Accuracy:\\n{accuracy_score(y_test, y_pred_nb)}\\n')\n",
    "print(f'Naive Bayes Confusion Matrix:\\n{confusion_matrix(y_test, y_pred_nb)}\\n')\n",
    "print(f'Naive Bayes Classification Report:\\n{classification_report(y_test, y_pred_nb, zero_division=1)}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support Vector Machines Accuracy:\n",
      "0.5566666666666666\n",
      "\n",
      "Support Vector Machines Confusion Matrix:\n",
      "[[  2   3   0   5  11]\n",
      " [  2   3   1   4   9]\n",
      " [  2   1   1   3   8]\n",
      " [  2   0   2   4  37]\n",
      " [  5   6   8  24 157]]\n",
      "\n",
      "Support Vector Machines Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.15      0.10      0.12        21\n",
      "           2       0.23      0.16      0.19        19\n",
      "           3       0.08      0.07      0.07        15\n",
      "           4       0.10      0.09      0.09        45\n",
      "           5       0.71      0.79      0.74       200\n",
      "\n",
      "    accuracy                           0.56       300\n",
      "   macro avg       0.26      0.24      0.24       300\n",
      "weighted avg       0.52      0.56      0.53       300\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Support Vector Machines\n",
    "\n",
    "svm = SVC(kernel='linear')\n",
    "svm.fit(X_train, y_train)\n",
    "y_pred_svm = svm.predict(X_test)\n",
    "\n",
    "# Print the accuracy, confusion matrix, and classification report\n",
    "print(f'Support Vector Machines Accuracy:\\n{accuracy_score(y_test, y_pred_svm)}\\n')\n",
    "print(f'Support Vector Machines Confusion Matrix:\\n{confusion_matrix(y_test, y_pred_svm)}\\n')\n",
    "print(f'Support Vector Machines Classification Report:\\n{classification_report(y_test, y_pred_svm, zero_division=1)}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy:\n",
      "0.6633333333333333\n",
      "\n",
      "Random Forest Confusion Matrix:\n",
      "[[  0   1   0   0  20]\n",
      " [  0   0   0   0  19]\n",
      " [  0   0   0   0  15]\n",
      " [  0   0   0   0  45]\n",
      " [  0   0   0   1 199]]\n",
      "\n",
      "Random Forest Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      0.00      0.00        21\n",
      "           2       0.00      0.00      0.00        19\n",
      "           3       1.00      0.00      0.00        15\n",
      "           4       0.00      0.00      0.00        45\n",
      "           5       0.67      0.99      0.80       200\n",
      "\n",
      "    accuracy                           0.66       300\n",
      "   macro avg       0.53      0.20      0.16       300\n",
      "weighted avg       0.57      0.66      0.53       300\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=100)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "\n",
    "# Print the accuracy, confusion matrix, and classification report\n",
    "print(f'Random Forest Accuracy:\\n{accuracy_score(y_test, y_pred_rf)}\\n')\n",
    "print(f'Random Forest Confusion Matrix:\\n{confusion_matrix(y_test, y_pred_rf)}\\n')\n",
    "print(f'Random Forest Classification Report:\\n{classification_report(y_test, y_pred_rf,zero_division=1)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
